{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to read and process the evaluation results from a CSV file\n",
    "def process_results(csv_file, model_name):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['Model'] = model_name\n",
    "    return df[['Model', 'Subpart', 'MSE', 'RMSE', 'MAE']]\n",
    "\n",
    "# Directory containing the evaluation results\n",
    "results_dir = \"../Predictions/Surface/\"\n",
    "\n",
    "# DataFrame to store the evaluation results for each subpart\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Iterate over the models in the directory\n",
    "for model_name in os.listdir(results_dir):\n",
    "    model_path = os.path.join(results_dir, model_name)\n",
    "    eval_file = os.path.join(model_path, \"eval.csv\")\n",
    "    try:\n",
    "        assert os.path.exists(eval_file)\n",
    "    except AssertionError:\n",
    "        print(f\"Missing evaluation results for {model_name}\")\n",
    "        continue\n",
    "    # Process the evaluation results for the current model\n",
    "    results = process_results(eval_file, model_name)\n",
    "    all_results = all_results.append(results, ignore_index=True)\n",
    "\n",
    "def get_nice_dataframe_sub(df, metrics, subpart = \"Full\", vertical = False):\n",
    "    models = df[\"Model\"].unique()\n",
    "    df = df.copy()\n",
    "    df = df[df[\"Subpart\"] == subpart]\n",
    "\n",
    "    # creates a dataframe where each metric has a column for its mean and std.\n",
    "    # the mean and std. are computed for each model\n",
    "    # the dataframe is then saved as a csv file\n",
    "    df_std = pd.DataFrame(columns=metrics)\n",
    "    for metric in metrics:\n",
    "        df_std[metric] = df.groupby([\"Model\"])[metric].std()\n",
    "\n",
    "    df_mean = pd.DataFrame(columns=metrics)\n",
    "    for metric in metrics:\n",
    "        df_mean[metric] = df.groupby([\"Model\"])[metric].mean()\n",
    "\n",
    "    df_mean = df_mean.round(2)\n",
    "    df_std = df_std.round(2)\n",
    "\n",
    "    #combine both dataframes, intercalating columns\n",
    "\n",
    "    empty_df = pd.DataFrame(columns = metrics)\n",
    "    for metric in metrics:\n",
    "        i = 0\n",
    "        for model in models:\n",
    "            mean_str = str(df_mean.loc[model, metric]) \n",
    "            std_str = str(df_std.loc[model, metric]) \n",
    "            if len(mean_str) == 3 or (mean_str[2] == '.' and len(mean_str) == 4):\n",
    "                mean_str += '0'\n",
    "            if len(std_str) == 3 or (std_str[2] == '.' and len(std_str) == 4):\n",
    "                std_str += '0'\n",
    "              \n",
    "            empty_df.loc[model, metric] = mean_str + \" (\" + std_str + \")\"\n",
    "            i+=1\n",
    "\n",
    "    # transposes the dataframe\n",
    "    if vertical:\n",
    "        empty_df = empty_df.T\n",
    "    \n",
    "    return empty_df\n",
    "\n",
    "metrics = ['MAE', 'MSE', 'RMSE']\n",
    "subparts = ['Full', 'LV', 'RV', 'LA', 'RA', 'aorta']\n",
    "\n",
    "for subpart in subparts:\n",
    "    print(subpart)\n",
    "    nice = get_nice_dataframe_sub(all_results, metrics, subpart, vertical=1)\n",
    "    display(nice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to read and process the evaluation results from a CSV file\n",
    "def process_results(csv_file, model_name):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df['Model'] = model_name\n",
    "    return df\n",
    "\n",
    "# Directory containing the evaluation results\n",
    "results_dir = \"../Predictions/Surface/\"\n",
    "\n",
    "# DataFrame to store the evaluation results for each subpart\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Iterate over the models in the directory\n",
    "for model_name in os.listdir(results_dir):\n",
    "    model_path = os.path.join(results_dir, model_name)\n",
    "    eval_file = os.path.join(model_path, \"metrics.csv\")\n",
    "\n",
    "    if not os.path.exists(eval_file):\n",
    "        continue\n",
    "    \n",
    "    # Process the evaluation results for the current model\n",
    "    results = process_results(eval_file, model_name)\n",
    "    all_results = all_results.append(results, ignore_index=True)\n",
    "\n",
    "def get_nice_dataframe(df, metrics, vertical = False):\n",
    "    models = df[\"Model\"].unique()\n",
    "\n",
    "    # creates a dataframe where each metric has a column for its mean and std.\n",
    "    # the mean and std. are computed for each model\n",
    "    # the dataframe is then saved as a csv file\n",
    "    df_std = pd.DataFrame(columns=metrics)\n",
    "    for metric in metrics:\n",
    "        df_std[metric] = df.groupby([\"Model\"])[metric].std()\n",
    "\n",
    "    df_mean = pd.DataFrame(columns=metrics)\n",
    "    for metric in metrics:\n",
    "        df_mean[metric] = df.groupby([\"Model\"])[metric].mean()\n",
    "\n",
    "    df_mean = df_mean.round(2)\n",
    "    df_std = df_std.round(2)\n",
    "\n",
    "    #combine both dataframes, intercalating columns\n",
    "\n",
    "    empty_df = pd.DataFrame(columns = metrics)\n",
    "    for metric in metrics:\n",
    "        i = 0\n",
    "        for model in models:\n",
    "            mean_str = str(df_mean.loc[model, metric]) \n",
    "            std_str = str(df_std.loc[model, metric]) \n",
    "            if len(mean_str) == 3 or (mean_str[2] == '.' and len(mean_str) == 4):\n",
    "                mean_str += '0'\n",
    "            if len(std_str) == 3 or (std_str[2] == '.' and len(std_str) == 4):\n",
    "                std_str += '0'\n",
    "              \n",
    "            empty_df.loc[model, metric] = mean_str + \" (\" + std_str + \")\"\n",
    "            i+=1\n",
    "\n",
    "    # transposes the dataframe\n",
    "    if vertical:\n",
    "        empty_df = empty_df.T\n",
    "    \n",
    "    return empty_df\n",
    "\n",
    "\n",
    "metrics = ['LV Endo - DC', 'LV Endo - HD', 'LV Endo - MCD', \n",
    "           'LV Myo - DC', 'LV Myo - HD', 'LV Myo - MCD', \n",
    "           'RV Endo - DC', 'RV Endo - HD', 'RV Endo - MCD']\n",
    "\n",
    "\n",
    "#metrics = [\"LA 2CH - DC\", \"LA 2CH - HD\", \"LA 2CH - MCD\",\n",
    "#            \"LA 4CH - DC\", \"LA 4CH - HD\", \"LA 4CH - MCD\",\n",
    "#            \"RA 4CH - DC\", \"RA 4CH - HD\", \"RA 4CH - MCD\"]\n",
    "\n",
    "nice = get_nice_dataframe(all_results, metrics, vertical=1)\n",
    "\n",
    "nice[\"MCSI-Net-Paper\"] = [\"0.88 (0.05)\", \"4.74 (1.75)\", \"1.86 (0.79)\",\n",
    "                    \"0.78 (0.08)\", \"4.75 (1.76)\", \"1.86 (0.82)\", \n",
    "                    \"0.85 (0.06)\", \"7.06 (2.64)\", \"2.27 (0.95)\"]\n",
    "\n",
    "display(nice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"LA 2CH - DC\", \"LA 2CH - HD\", \"LA 2CH - MCD\",\n",
    "            \"LA 4CH - DC\", \"LA 4CH - HD\", \"LA 4CH - MCD\",\n",
    "            \"RA 4CH - DC\", \"RA 4CH - HD\", \"RA 4CH - MCD\"]\n",
    "\n",
    "\n",
    "# Directory containing the evaluation results\n",
    "results_dir = \"../Predictions/Surface/\"\n",
    "\n",
    "# DataFrame to store the evaluation results for each subpart\n",
    "all_results = pd.DataFrame()\n",
    "\n",
    "# Iterate over the models in the directory\n",
    "for model_name in os.listdir(results_dir):\n",
    "    model_path = os.path.join(results_dir, model_name)\n",
    "    eval_file = os.path.join(model_path, \"lax_metrics_yan.csv\")\n",
    "\n",
    "    if not os.path.exists(eval_file):\n",
    "        continue\n",
    "    \n",
    "    # Process the evaluation results for the current model\n",
    "    results = process_results(eval_file, model_name)\n",
    "    all_results = all_results.append(results, ignore_index=True)\n",
    "    \n",
    "nice = get_nice_dataframe(all_results, metrics, vertical=1)\n",
    "\n",
    "display(nice)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2f31d7e842d4c02a4fdd8a49a54c27977ffbe51e51f3f2334136391596cbe6a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
